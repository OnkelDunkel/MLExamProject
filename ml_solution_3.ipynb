{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Unnamed: 0',\n",
       " 'Unnamed: 0.1',\n",
       " 'company',\n",
       " 'location',\n",
       " 'dates',\n",
       " 'job-title',\n",
       " 'summary',\n",
       " 'pros',\n",
       " 'cons',\n",
       " 'advice-to-mgmt',\n",
       " 'overall-ratings',\n",
       " 'work-balance-stars',\n",
       " 'culture-values-stars',\n",
       " 'carrer-opportunities-stars',\n",
       " 'comp-benefit-stars',\n",
       " 'senior-mangemnet-stars',\n",
       " 'helpful-count',\n",
       " 'link',\n",
       " 'summary_processed',\n",
       " 'summary_char_length',\n",
       " 'summary_word_count',\n",
       " 'summary_stopword_count',\n",
       " 'summary_stopword_freq',\n",
       " 'pros_processed',\n",
       " 'pros_char_length',\n",
       " 'pros_word_count',\n",
       " 'pros_stopword_count',\n",
       " 'pros_stopword_freq',\n",
       " 'cons_processed',\n",
       " 'cons_char_length',\n",
       " 'cons_word_count',\n",
       " 'cons_stopword_count',\n",
       " 'cons_stopword_freq',\n",
       " 'text',\n",
       " 'text_processed',\n",
       " 'text_char_length',\n",
       " 'text_word_count',\n",
       " 'text_stopword_count',\n",
       " 'text_stopword_freq']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"all_processed2.csv\")\n",
    "df.head(3)\n",
    "df.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the label into a binary value: 1 if rating is 4-5, 0 if rating is 1-3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"label\"] = df.apply(lambda row: 1 if row[\"overall-ratings\"] >= 4 else 0, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balancing out the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost more than twice as many positive as negative reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive: 45607, Negative: 21792\n"
     ]
    }
   ],
   "source": [
    "df_pos = df[df[\"label\"] == 1]\n",
    "df_neg = df[df[\"label\"] == 0]\n",
    "print(\"Positive: {0}, Negative: {1}\".format(df_pos.count()[0], df_neg.count()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Undersampling of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos_res = resample(df_pos, \n",
    "                   n_samples=df_neg.count()[0], \n",
    "                   random_state=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21792"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pos_res.count()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_neg.append(df_pos_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the culomns containing measurements about the text like number of words/stopwords, stopwords frequency or length in characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pros_word_count</th>\n",
       "      <th>pros_stopword_freq</th>\n",
       "      <th>pros_char_length</th>\n",
       "      <th>cons_word_count</th>\n",
       "      <th>cons_stopword_freq</th>\n",
       "      <th>cons_char_length</th>\n",
       "      <th>summary_word_count</th>\n",
       "      <th>summary_stopword_freq</th>\n",
       "      <th>summary_char_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>38.0</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>228.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>104.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>31.0</td>\n",
       "      <td>0.354839</td>\n",
       "      <td>178.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.402985</td>\n",
       "      <td>377.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>30.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>76.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>9.0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>47.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>344.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>18.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>111.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.365854</td>\n",
       "      <td>247.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    pros_word_count  pros_stopword_freq  pros_char_length  cons_word_count  \\\n",
       "30             38.0            0.315789             228.0             13.0   \n",
       "37             31.0            0.354839             178.0             67.0   \n",
       "41              6.0            0.500000              30.0             12.0   \n",
       "48              9.0            0.111111              47.0             57.0   \n",
       "54             18.0            0.333333             111.0             41.0   \n",
       "\n",
       "    cons_stopword_freq  cons_char_length  summary_word_count  \\\n",
       "30            0.153846             104.0                 2.0   \n",
       "37            0.402985             377.0                 6.0   \n",
       "41            0.583333              76.0                 2.0   \n",
       "48            0.315789             344.0                 1.0   \n",
       "54            0.365854             247.0                 7.0   \n",
       "\n",
       "    summary_stopword_freq  summary_char_length  \n",
       "30               0.000000                 15.0  \n",
       "37               0.000000                 34.0  \n",
       "41               0.000000                  8.0  \n",
       "48               0.000000                  3.0  \n",
       "54               0.714286                 34.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_measures_indices = [\n",
    "    \"pros_word_count\", \"pros_stopword_freq\", \"pros_char_length\",\n",
    "    \"cons_word_count\", \"cons_stopword_freq\", \"cons_char_length\",\n",
    "    \"summary_word_count\", \"summary_stopword_freq\", \"summary_char_length\"\n",
    "]\n",
    "\n",
    "X_txt_measures = df[txt_measures_indices]\n",
    "X_txt_measures.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We tested the text measurements' preditcion value with a couple of different models using Scikit-Learns cross_val_score function for cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=10, shuffle=True, random_state=4)\n",
    "\n",
    "def do_cross_val(model):\n",
    "    start_time = time.time()\n",
    "    acc_score = cross_val_score(model, X_txt_measures, y, cv=kf)\n",
    "    print(acc_score)\n",
    "    print(np.mean(acc_score))\n",
    "    print(\"It took a total of {0} minutes\"\n",
    "          .format((time.time() - start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.660702   0.65290204 0.6673549  0.66551961 0.66727857 0.65144562\n",
      " 0.6608536  0.65442864 0.65993575 0.67370353]\n",
      "0.661412425916702\n",
      "It took a total of 0.04740280310312907 minutes\n"
     ]
    }
   ],
   "source": [
    "pipe_knn = make_pipeline(StandardScaler(with_mean=False), KNeighborsClassifier(n_neighbors=10))\n",
    "acc = do_cross_val(pipe_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.68570773 0.68203716 0.68891948 0.69694884 0.69573199 0.68815971\n",
      " 0.69274897 0.68150528 0.67875172 0.69710877]\n",
      "0.6887619639005498\n",
      "It took a total of 0.026148144404093424 minutes\n"
     ]
    }
   ],
   "source": [
    "pipe_lr = make_pipeline(StandardScaler(with_mean=False), LogisticRegression(random_state=21, solver='liblinear', C=1))\n",
    "do_cross_val(pipe_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.68111952 0.67882542 0.69098417 0.68869007 0.68081689 0.69412575\n",
      " 0.69137219 0.68701239 0.68540615 0.70330427]\n",
      "0.6881656810651972\n",
      "It took a total of 0.03066873550415039 minutes\n"
     ]
    }
   ],
   "source": [
    "pipe_dt = make_pipeline(StandardScaler(with_mean=False), DecisionTreeClassifier(max_depth=45, random_state=21))\n",
    "do_cross_val(pipe_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.59233769 0.5815554  0.58729066 0.60082588 0.60578247 0.58834328\n",
      " 0.58352455 0.59637448 0.58834328 0.60256999]\n",
      "0.5926947680212199\n",
      "It took a total of 0.002381602923075358 minutes\n"
     ]
    }
   ],
   "source": [
    "pipe_nb = make_pipeline(StandardScaler(with_mean=False), GaussianNB())\n",
    "do_cross_val(pipe_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We used the TfidVectorizer from sci-kit learn to transform the strings into word vectors. We chose to only vectorize the 3000 most common words for simplicity sake. In total there was around 21,000 unique words in all the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "no_of_words = 3000\n",
    "tfidf = TfidfVectorizer(max_features=no_of_words, ngram_range=(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43584, 3000)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_txt_vecs = tfidf.fit_transform(df[\"text\"].tolist())\n",
    "X_txt_vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_txt_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now comes the task of combining the text vectors with the text measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pros_word_count',\n",
       " 'pros_stopword_freq',\n",
       " 'pros_char_length',\n",
       " 'cons_word_count',\n",
       " 'cons_stopword_freq',\n",
       " 'cons_char_length',\n",
       " 'summary_word_count',\n",
       " 'summary_stopword_freq',\n",
       " 'summary_char_length']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_txt_measures.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([   30,    37,    41,    48,    54,    61,    66,    83,    90,\n",
       "               92,\n",
       "            ...\n",
       "             1665,   861, 36104, 41078, 45657, 58115, 12708, 61485,  8717,\n",
       "            18851],\n",
       "           dtype='int64', length=43584)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_txt_measures.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_full = pd.concat([X_txt_measures,pd.DataFrame(\n",
    "            index = X_txt_measures.index, \n",
    "            data = X_txt_vecs.toarray()\n",
    "        )], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pros_word_count</th>\n",
       "      <th>pros_stopword_freq</th>\n",
       "      <th>pros_char_length</th>\n",
       "      <th>cons_word_count</th>\n",
       "      <th>cons_stopword_freq</th>\n",
       "      <th>cons_char_length</th>\n",
       "      <th>summary_word_count</th>\n",
       "      <th>summary_stopword_freq</th>\n",
       "      <th>summary_char_length</th>\n",
       "      <th>0</th>\n",
       "      <th>...</th>\n",
       "      <th>2990</th>\n",
       "      <th>2991</th>\n",
       "      <th>2992</th>\n",
       "      <th>2993</th>\n",
       "      <th>2994</th>\n",
       "      <th>2995</th>\n",
       "      <th>2996</th>\n",
       "      <th>2997</th>\n",
       "      <th>2998</th>\n",
       "      <th>2999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>38.0</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>228.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>104.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>31.0</td>\n",
       "      <td>0.354839</td>\n",
       "      <td>178.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.402985</td>\n",
       "      <td>377.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.115659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>30.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>76.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 3009 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    pros_word_count  pros_stopword_freq  pros_char_length  cons_word_count  \\\n",
       "30             38.0            0.315789             228.0             13.0   \n",
       "37             31.0            0.354839             178.0             67.0   \n",
       "41              6.0            0.500000              30.0             12.0   \n",
       "\n",
       "    cons_stopword_freq  cons_char_length  summary_word_count  \\\n",
       "30            0.153846             104.0                 2.0   \n",
       "37            0.402985             377.0                 6.0   \n",
       "41            0.583333              76.0                 2.0   \n",
       "\n",
       "    summary_stopword_freq  summary_char_length    0  ...  2990  2991  2992  \\\n",
       "30                    0.0                 15.0  0.0  ...   0.0   0.0   0.0   \n",
       "37                    0.0                 34.0  0.0  ...   0.0   0.0   0.0   \n",
       "41                    0.0                  8.0  0.0  ...   0.0   0.0   0.0   \n",
       "\n",
       "    2993  2994  2995  2996  2997  2998      2999  \n",
       "30   0.0   0.0   0.0   0.0   0.0   0.0  0.000000  \n",
       "37   0.0   0.0   0.0   0.0   0.0   0.0  0.115659  \n",
       "41   0.0   0.0   0.0   0.0   0.0   0.0  0.000000  \n",
       "\n",
       "[3 rows x 3009 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_full.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We devided the data into training and testing data but then got away from it aftwards since we are using cross_val_score. However we kept the code to be easily implemented by just commenting out 2 lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_full, \n",
    "                                                    y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=22)\n",
    "X_train = X_full\n",
    "y_train = y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We created a function to easily run cross_val_score on the model with the correct data and print out the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=10, shuffle=True, random_state=4)\n",
    "\n",
    "def do_cross_val(model):\n",
    "    start_time = time.time()\n",
    "    acc_score = cross_val_score(model, X_train, y_train, cv=kf)\n",
    "    print(acc_score)\n",
    "    print(np.mean(acc_score))\n",
    "    print(\"It took a total of {0} minutes\"\n",
    "          .format((time.time() - start_time)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We ran with various different models trying to tweak the settings for best result. The 2 best performing models ended up being Logistic Regression and Naive Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic regression was ultimately the best of for our data. Logistic Regression performed best with normalized data and with the n_grams in the range of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.77632485 0.76554256 0.77448956 0.7687543  0.76686553 0.77214318\n",
      " 0.77374943 0.76755392 0.76916017 0.76961909]\n",
      "0.7704202590347854\n",
      "It took a total of 1.2042159597078959 minutes\n"
     ]
    }
   ],
   "source": [
    "pipe_lr = make_pipeline(StandardScaler(with_mean=False), LogisticRegression(random_state=21, solver='liblinear', C=1))\n",
    "do_cross_val(pipe_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The naives bayes performed best with unscaled data and with the n_grams in the range of 1-3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.73411333 0.72746043 0.73044276 0.73503097 0.73634695 0.732905\n",
      " 0.73588802 0.72785682 0.73129876 0.73474071]\n",
      "0.7326083743118872\n",
      "It took a total of 0.5234104514122009 minutes\n"
     ]
    }
   ],
   "source": [
    "pipe_nb = make_pipeline(GaussianNB())\n",
    "do_cross_val(pipe_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ngram_1,1 3000 mixed standardscaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We wanted to see if we could improve the result using ensambled learning to combine the results of the 2 models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first tried splitting the data into text vectors for the Naive Bayes and text measurements for the Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_txt_measures(X):\n",
    "    return X[:, :9]\n",
    "\n",
    "def select_txt_vectors(X):\n",
    "    return X[:, 9:]\n",
    "\n",
    "pipe_lr = make_pipeline(\n",
    "    FunctionTransformer(select_txt_measures, validate = True),\n",
    "    StandardScaler(with_mean=False), \n",
    "    LogisticRegression(random_state=21, solver='liblinear', C=1)\n",
    ")\n",
    "pipe_nb = make_pipeline(\n",
    "    FunctionTransformer(select_txt_vectors, validate = True),\n",
    "    GaussianNB()\n",
    ")\n",
    "\n",
    "voting_c = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('lr', pipe_lr), \n",
    "        ('rf', pipe_nb),\n",
    "    ],\n",
    "    voting='soft'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.72356045 0.71690755 0.71025465 0.71621932 0.72303809 0.71844883\n",
      " 0.72418541 0.72028453 0.720514   0.71294172]\n",
      "0.718635453374044\n",
      "It took a total of 0.5847811381022135 minutes\n"
     ]
    }
   ],
   "source": [
    "do_cross_val(voting_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then tried feeding both pipelines the same data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_lr = make_pipeline(\n",
    "    StandardScaler(with_mean=False), \n",
    "    LogisticRegression(random_state=21, solver='liblinear', C=1)\n",
    ")\n",
    "pipe_nb = make_pipeline(\n",
    "    GaussianNB()\n",
    ")\n",
    "\n",
    "voting_c = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('lr', pipe_lr), \n",
    "        ('rf', pipe_nb),\n",
    "    ],\n",
    "    voting='soft'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.73411333 0.73021335 0.73067217 0.73617802 0.73749426 0.73474071\n",
      " 0.7372648  0.72923359 0.732905   0.73818265]\n",
      "0.7340997894246115\n",
      "It took a total of 1.6498730023701986 minutes\n"
     ]
    }
   ],
   "source": [
    "do_cross_val(voting_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oddly enough the voting classifier seems to follow the Naive Bayes model accuracy quite closely. It did not give us a better result than when running with the Logistic Regression alone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We managed to reach an accuracy of close to 80% so we are quite satisfied with the result. We might be able to improve the predictions by using a neural network but that will be for some other time.\n",
    "However we learned a lot about Natural Language Processing and we think we will be able to do better in the future. Maybe the result could also be improved by extracting extra features like special character count etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
